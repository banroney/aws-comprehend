{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehend Custom Classifier using BBC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise below we will do the following: We will create a custom classifier for classifying news documents into sections such as Politics, Business, Entertainment, Sports, etc. We will be using a trained data set from BBC new pubicly available. In order to achieve that, we will have to build a custom classifier in Comprehend. We can later use the custom classifier for real time analysis later in the exercise using an endpoint. One end point can be related to one model. \n",
    "\n",
    "This is a summary of the steps done below\n",
    "\n",
    "   #### Train\n",
    "    1. Get the sample data from BBC Bews\n",
    "    2. Format and preprocess the data as per the needs of Comprehend\n",
    "    3. Upload the data to S3\n",
    "    4. Make sure the notebook role and the Comprehend role has the requisite permissions\n",
    "    5. Start the training job\n",
    "   \n",
    "   #### Test/Use\n",
    "    1. Build a test data set\n",
    "    2. Format and preprocess the data as per the needs of Comprehend\n",
    "    3. Uploadpload the data to S3\n",
    "    4. Make sure the notebook role and the Comprehend role has the requisite permissions\n",
    "    5. Start the prediction job\n",
    "    \n",
    "   #### Validate\n",
    "      1. Download the results/prediction outputs\n",
    "      2. Load in into a dataframe\n",
    "      3. Match it with your version of truth\n",
    "      4. Check accuracy/precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Labeled dataset from BBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download a sample dataset and unzip its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf bbc\n",
    "!unzip -o bbc-fulltext.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the BBC dataset into Amazon Comprehend supported format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages. Note that while using Amazon Comprehend, we dont need any ML frameworks or libraries. All of the ML workload is executed by the managed service. This notebook is a mere UI for executing the backend services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from os.path import normpath, basename\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataframe object and setting the source path for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "mapping = {}\n",
    "source_path = \"bbc/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the directory and finding out the trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(os.listdir(source_path)):\n",
    "    if os.path.isdir(source_path+i):\n",
    "        mapping[i] = sorted(os.listdir(source_path+i))[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to identify or preprocess stop words, lexicons, or . Amzon Comprehend will take care of the those. We sipmply replace the new lines characters, just so our data is more readable in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "data = []\n",
    "for i,j in mapping.items():\n",
    "    for k in j:\n",
    "        label.append(i)\n",
    "        data.append(open(source_path+i+\"/\" + k,encoding=\"cp1252\").read().replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A sample label {} : {}\".format(label[:1],data[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = label\n",
    "df[\"document\"] = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the dataframe to bring in randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training data set to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the client to access the bucket and upload the training files to the bucket. Make sure that roles for comprehend and this notebook both have read access to the training data. The notebook needs to have write access to training and testing locations in order to upload them. The Comprehend role needs to have write access to the output locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the bucket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifier = 'BBCNewsDataClassifier'\n",
    "BUCKET = 'comprehend-demo-20200602'\n",
    "s3_file = data_classifier + '/train/train.csv'\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('train.csv', BUCKET, s3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and start training job for custom classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job can be started asynchronously with a name. Status can be retrieved later using the ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Boto3 Client\n",
    "client = boto3.client('comprehend', region_name='us-east-1')\n",
    "role = 'arn:aws:iam::951145066533:role/service-role/AmazonComprehendServiceRole-_908203_cmpd'\n",
    "\n",
    "# Create a custom document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': 's3://'+ BUCKET + '/' + s3_file\n",
    "    },\n",
    "    DataAccessRoleArn=role,\n",
    "    DocumentClassifierName=data_classifier,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(\"Create response: %s\\n\", create_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_response['DocumentClassifierArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the status of the classifier\n",
    "classifierArn = create_response['DocumentClassifierArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_document_classifier = client.describe_document_classifier(\n",
    "        DocumentClassifierArn = classifierArn\n",
    "    )\n",
    "    status = describe_document_classifier[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    print(\"Custom entity recognizer: {}\".format(status))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "\n",
    "if time.time() > max_time:\n",
    "    client.stop_training_document_classifier(\n",
    "        DocumentClassifierArn = classifierArn\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing all the classifiers in the account/region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all classifiers in account\n",
    "list_response = client.list_document_classifiers()\n",
    "for item in list_response['DocumentClassifierPropertiesList']:\n",
    "    eval_metrics = item['ClassifierMetadata']['EvaluationMetrics']\n",
    "    print('{}:\\n Precision {} Recall {} Accuracy {} F1 {}'.format(item['DocumentClassifierArn'],\n",
    "                                                         eval_metrics['Precision'],\n",
    "                                                         eval_metrics['Recall'],\n",
    "                                                         eval_metrics['Accuracy'],\n",
    "                                                         eval_metrics['F1Score']\n",
    "                                                        )\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Custom Classifier Job for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the test dataframe by sample a random set from 301 to 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Test Data\n",
    "df_test = pd.DataFrame()\n",
    "df_truth = pd.DataFrame()\n",
    "\n",
    "test_mapping = {}\n",
    "for i in sorted(os.listdir(source_path)):\n",
    "    if os.path.isdir(source_path+i):\n",
    "        test_mapping[i] = sorted(os.listdir(source_path+i))[201:300]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing 2 data frames - one for testing the labels - one for truth labels. The idea here is to test the results, but this can also be used to run actual jobs. Amazon Comprehend doesnt need any testing. The managed service is already tested and the confusion metric results like recall, precision and F1 score are already posted in the console. These re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "truth_label = []\n",
    "\n",
    "for i,j in test_mapping.items():\n",
    "    for k in j:\n",
    "        truth_label.append(i)\n",
    "        test_data.append(open(source_path+i+\"/\" + k,encoding=\"cp1252\").read().replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truth df has both document and label while the test df has only document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"document\"] = test_data\n",
    "df_truth[\"label\"] = truth_label\n",
    "df_truth[\"document\"] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing randomness into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "df_truth = df_truth.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to CSV and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv and upload to S3\n",
    "df_test.to_csv(\"test.csv\",index=False,header=False)\n",
    "df_truth.to_csv(\"truth.csv\",index=False,header=False)\n",
    "\n",
    "s3_file_test = data_classifier + '/test/test.csv'\n",
    "s3_file_truth = data_classifier + '/output/truth.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('test.csv', BUCKET, s3_file_test)\n",
    "s3.upload_file('truth.csv', BUCKET, s3_file_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Classification Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = data_classifier + \"_\" + (datetime.date.today().strftime(\"%m%d%Y\"))\n",
    "s3_file_results = data_classifier + 'output/results_' + job_name\n",
    "\n",
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': 's3://'+ BUCKET + '/' + s3_file_test,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': 's3://' + BUCKET + '/' +  s3_file_results\n",
    "    },\n",
    "    JobName=job_name,\n",
    "    DataAccessRoleArn='arn:aws:iam::951145066533:role/service-role/AmazonComprehendServiceRole-_908203_cmpd',\n",
    "    DocumentClassifierArn='arn:aws:comprehend:us-east-1:951145066533:document-classifier/BBCNewsDataClassifier'\n",
    ")\n",
    "\n",
    "print(\"Start response: %s\\n\", start_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using this function to read s3 locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_path(s3_path):\n",
    "    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "    bucket=path_parts.pop(0)\n",
    "    key=\"/\".join(path_parts)\n",
    "    return bucket, key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the job until completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    job_response = client.describe_document_classification_job(\n",
    "        JobId=start_response['JobId']\n",
    "    )\n",
    "    status = job_response['DocumentClassificationJobProperties']['JobStatus']\n",
    "    print(\"Custom entity recognizer: {}\".format(status))\n",
    "    \n",
    "    if status == \"COMPLETED\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the S3 Location of the output and download the file to the notebook location and unzip the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = job_response['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri']\n",
    "local_output_file_name = job_response['DocumentClassificationJobProperties']['JobName'] + '_' + basename(normpath(output_file_path))\n",
    "print(local_output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_1, key = split_s3_path(output_file_path)\n",
    "s3.download_file(bucket_1,key,local_output_file_name)\n",
    "\n",
    "!tar zxvf BBCNewsDataClassifier_06032020_output.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DF from the test results csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabels = []\n",
    "with open('predictions.jsonl','r') as f:\n",
    "    for i in f:\n",
    "        j = json.loads(i)['Classes']\n",
    "        predictedLabels.append(j[0]['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results = pd.read_csv(\"test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results.columns = [\"document\"]\n",
    "df_test_results[\"PredictedLabel\"] = predictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the prediction results with the truth results to match accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeDf = pd.merge(df_test_results,df_truth,on=[\"document\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the difference of unmatched labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_match = mergeDf[mergeDf.PredictedLabel == mergeDf.label].count()\n",
    "count_mismatch = mergeDf[mergeDf.PredictedLabel != mergeDf.label].count()\n",
    "\n",
    "print(\"{} documents were predicted with the right results and \\\n",
    "      \\n{} documents were predicted with the wrong results\".\n",
    "      format(count_match['document'],count_mismatch['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
