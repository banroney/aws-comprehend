{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How to prepare a dataset and submit a custom entity in Amazon Comprehend\n",
    "\n",
    "This notebook covers how to prepare a training dataset for custom entities in Amazon Comprehend\n",
    "\n",
    "More information on how to create a custom entity recognizer model can be found here.\n",
    "\n",
    "https://docs.aws.amazon.com/comprehend/latest/dg/training-recognizers.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import csv\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be using the following tweet dataset. https://www.kaggle.com/thoughtvector/customer-support-on-twitter First lets get our data and process it to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_NER_PREFIX = 'TweetTelco'\n",
    "BUCKET = 'comprehend-ner-20200602'\n",
    "role = 'arn:aws:iam::951145066533:role/service-role/AmazonComprehendServiceRole-cmpdner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_file = 'data/tweet_telco.csv'\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(BUCKET,s3_file,s3_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['text'] \n",
    "tweets = pd.read_csv('data/tweet_telco.csv',encoding='utf-8',names=colnames, header=None)\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create our dataset we need to provide an entity list for our new class named NEGATIVITY.\n",
    "\n",
    "In order to find relevant entities, we used another blazingtext to find similar words using word2vec. See ./blazingtext_word2vec/blazingtext_word2vec_telco_tweets.ipynb for examples of retrieving such keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = ['Really', 'cheated', 'annoyed', 'unhelpful', 'frustrated', 'upset' , 'unhappy', 'angry', 'badly', 'bad', 'surprised', 'sadly', 'dissatisfied', 'disappointed', 'disgusted']\n",
    "\n",
    "df_entity_list = pd.DataFrame(negative_words, columns=['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another column with our class label. This is required part of the Amazon Comprehend training dataset.\n",
    "\n",
    "More information can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/comprehend/latest/dg/cer-entity-list.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_list['Type'] = 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'].to_csv('data/raw_negative.csv', encoding='utf-8', index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/raw_negative.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the entity list file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_list.to_csv('data/entity_negative_list.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/entity_negative_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_file = 'data/entity_negative_list.csv'\n",
    "s3.upload_file(entity_file, BUCKET, entity_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test file from our original telco tweet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/telco_negative_test.csv'\n",
    "tweets['text'].tail(10000).to_csv(train_file, encoding='utf-8', index=False,header=False)\n",
    "s3.upload_file(train_file, BUCKET, train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'data/telco_negative_test.csv'\n",
    "tweets['text'].head(10000).to_csv(test_file, encoding='utf-8', index=False,header=False)\n",
    "s3.upload_file(test_file, BUCKET, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the custom NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_entity_key = entity_file\n",
    "s3_train_key = train_file\n",
    "s3_test_key = test_file\n",
    "\n",
    "prefix = CUSTOM_NER_PREFIX\n",
    "\n",
    "#Create s3 paths\n",
    "s3_train_data = 's3://{}/{}'.format(BUCKET, s3_train_key)\n",
    "s3_train_entity = 's3://{}/{}'.format(BUCKET, s3_entity_key)\n",
    "s3_test_data = 's3://{}/{}'.format(BUCKET, s3_test_key)\n",
    "s3_output_test_data = 's3://{}/{}/test/{}'.format(BUCKET, prefix, \"telco_test_output.json\")\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Boto3 Client\n",
    "comprehend = boto3.client('comprehend', region_name='us-east-1')\n",
    "\n",
    "custom_entity_request = {\n",
    "\n",
    "      \"Documents\": { \n",
    "         \"S3Uri\": s3_train_data\n",
    "      },\n",
    "      \"EntityList\": { \n",
    "         \"S3Uri\": s3_train_entity\n",
    "      },\n",
    "      \"EntityTypes\": [ \n",
    "         { \n",
    "            \"Type\": \"NEGATIVE\"\n",
    "         }\n",
    "      ]\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "# Create a document classifier\n",
    "id = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "create_custom_entity_response = comprehend.create_entity_recognizer(\n",
    "        RecognizerName = CUSTOM_NER_PREFIX+id, \n",
    "        DataAccessRoleArn = role,\n",
    "        InputDataConfig = custom_entity_request,\n",
    "        LanguageCode = \"en\"\n",
    ")\n",
    "print(\"Create response: %s\\n\", create_custom_entity_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_custom_entity_response['EntityRecognizerArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the classifier\n",
    "describe_response = comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=create_custom_entity_response['EntityRecognizerArn'])\n",
    "print(\"Describe response: %s\", describe_response['EntityRecognizerProperties']['Status'])\n",
    "\n",
    "jobArn = create_custom_entity_response['EntityRecognizerArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_custom_recognizer = comprehend.describe_entity_recognizer(\n",
    "        EntityRecognizerArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_recognizer[\"EntityRecognizerProperties\"][\"Status\"]\n",
    "    print(\"Custom entity recognizer: {}\".format(status))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(describe_custom_recognizer[\"EntityRecognizerProperties\"][\"RecognizerMetadata\"][\"EntityTypes\"], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = comprehend.start_entities_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_test_data,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_output_test_data\n",
    "    },\n",
    "    DataAccessRoleArn=role,\n",
    "    JobName='Custom_Negative_Test',\n",
    "    EntityRecognizerArn=jobArn,\n",
    "    LanguageCode='en'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId = test_response['JobId']\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_job = comprehend.describe_entities_detection_job(\n",
    "        JobId = jobId\n",
    "    )\n",
    "    status = describe_job[\"EntitiesDetectionJobProperties\"][\"JobStatus\"]\n",
    "    print(\"Job Status: {}\".format(status))\n",
    "    \n",
    "    if status == \"COMPLETED\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the test output to local machine\n",
    "job_output = describe_job[\"EntitiesDetectionJobProperties\"][\"OutputDataConfig\"][\"S3Uri\"]\n",
    "path_prefix = 's3://{}/'.format(BUCKET)\n",
    "job_key = os.path.relpath(job_output, path_prefix)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(BUCKET).download_file(job_key, 'output.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the Entities values in a list\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for line in open('output', 'r'):\n",
    "    entities = json.loads(line)['Entities']\n",
    "    if entities != None and len(entities) > 0:\n",
    "        data.append(entities[0]['Text'])\n",
    "    \n",
    "\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "      \n",
    "    # insert the list to the set \n",
    "    list_set = set(list1) \n",
    "    # convert the set to the list \n",
    "    unique_list = (list(list_set)) \n",
    "    for x in unique_list: \n",
    "        print(x), \n",
    "        \n",
    "unique(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and use an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
